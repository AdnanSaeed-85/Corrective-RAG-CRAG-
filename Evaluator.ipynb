{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "530a4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from typing import List, TypedDict\n",
    "from CONFIG import OPENAI_EMBEDDED_MODEL, GROQ_MODEL\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import re\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "693bc7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = ChatGroq(model=GROQ_MODEL)\n",
    "embedded_model = OpenAIEmbeddings(model=OPENAI_EMBEDDED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ab70d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loading = PyPDFLoader(file_path='A:\\AI_Projects_Practice\\CRAG\\The_Evolution_of_AI_in_Dubai.pdf')\n",
    "pdf = pdf_loading.load()\n",
    "len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55ae38c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitting = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100).split_documents(pdf)\n",
    "len(splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed66f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = FAISS.from_documents(splitting, embedded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d59de87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boosting the economy, and improving quality of life for residents and visitors.\n",
      "Key components of this strategy include:\n",
      "Infrastructural Development: Dubai is investing in tech infrastructure to support AI-driven\n",
      "services. This entails deploying high-speed internet and cloud computing resources across the\n",
      "city.\n",
      "****************************************************************************************************\n",
      "Dubai AI Strategy: Introduced in 2017, this strategy aims to make Dubai a global hub for AI by\n",
      "2031. It focuses on utilizing AI to optimize government operations, improve city services, and\n",
      "advance economic development. It involves collaboration between governmental bodies and\n",
      "private sectors to create an ecosystem that fosters AI innovation.\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "retrieved = vector_storage.as_retriever(search_type='similarity', search_kwargs={'k': 2})\n",
    "for i in retrieved.invoke('dubai'):\n",
    "    print(i.page_content)\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62db02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPPER_THD = 0.7\n",
    "LOWER_THD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b2bd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class state(TypedDict):\n",
    "    question: str\n",
    "    doc: List[Document]\n",
    "\n",
    "    good_docs: List[Document]\n",
    "    verdict: str\n",
    "    reason: str\n",
    "\n",
    "    strip: List[str]\n",
    "    refined_strip: List[str]\n",
    "    refined_text: str\n",
    "    \n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa7a4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieved_node(state):\n",
    "    return {'doc': retrieved.invoke(state['question'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATOR\n",
    "\n",
    "class DocEvaluator(BaseModel):\n",
    "    score: float\n",
    "    reason: str\n",
    "\n",
    "doc_eval_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a strict retrieval evaluator for RAG.\\n\"\n",
    "            \"You will be given ONE retrieved chunk and a question.\\n\"\n",
    "            \"Return a relevance score in [0.0, 1.0].\\n\"\n",
    "            \"- 1.0: chunk alone is sufficient to answer fully/mostly\\n\"\n",
    "            \"- 0.0: chunk is irrelevant\\n\"\n",
    "            \"Be conservative with high scores.\\n\"\n",
    "            \"Also return a short reason.\\n\"\n",
    "            \"Output JSON only.\"\n",
    "\t  ),\n",
    "\t  (\n",
    "\t\t\"human\",\n",
    "\t\t\"Question is: {question}\\n\\nChunk is: {chunk}\"\n",
    "\t  )\n",
    "    ]\n",
    ")\n",
    "\n",
    "doc_eval_chain = doc_eval_prompt | llm.with_structured_output(DocEvaluator)\n",
    "\n",
    "def eval_each_doc_node(state: state) -> state:\n",
    "\n",
    "    q = state[\"question\"]\n",
    "    \n",
    "    scores: List[float] = []\n",
    "    reasons: List[str] = []\n",
    "    good: List[Document] = []\n",
    "\n",
    "    for d in state[\"docs\"]:\n",
    "        out = doc_eval_chain.invoke({\"question\": q, \"chunk\": d.page_content})\n",
    "        scores.append(out.score)\n",
    "        reasons.append(out.reason)\n",
    "\n",
    "        # 5) for CORRECT case we will refine only docs with score > LOWER_TH\n",
    "        if out.score > LOWER_THD:\n",
    "            good.append(d)\n",
    "\n",
    "    # 2) CORRECT if at least one doc > UPPER_TH\n",
    "    if any(s > UPPER_THD for s in scores):\n",
    "        return {\n",
    "            \"good_docs\": good,\n",
    "            \"verdict\": \"CORRECT\",\n",
    "            \"reason\": f\"At least one retrieved chunk scored > {UPPER_THD}.\",\n",
    "        }\n",
    "    \n",
    "        # 3) INCORRECT if all docs < LOWER_TH\n",
    "    if len(scores) > 0 and all(s < LOWER_THD for s in scores):\n",
    "        why = \"No chunk was sufficient.\"\n",
    "        return {\n",
    "            \"good_docs\": [],\n",
    "            \"verdict\": \"INCORRECT\",\n",
    "            \"reason\": f\"All retrieved chunks scored < {LOWER_THD}. {why}\",\n",
    "        }\n",
    "\n",
    "    # 4) Anything in between => AMBIGUOUS\n",
    "    why = \"Mixed relevance signals.\"\n",
    "    return {\n",
    "        \"good_docs\": good,\n",
    "        \"verdict\": \"AMBIGUOUS\",\n",
    "        \"reason\": f\"No chunk scored > {UPPER_THD}, but not all were < {LOWER_THD}. {why}\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f2aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc4ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
