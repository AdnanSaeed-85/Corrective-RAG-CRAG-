{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e300dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, TypedDict, Literal\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "from CONFIG import GROQ_MODEL\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1a8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = PyPDFLoader(file_path='A:\\AI_Projects_Practice\\CRAG\\The_Evolution_of_AI_in_Dubai.pdf').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8fc8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=150).split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c33e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce627e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=GROQ_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a631fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPPER_TH = 0.7\n",
    "LOWER_TH = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25165c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "\n",
    "    question: str\n",
    "    docs: List[Document]\n",
    "\n",
    "    good_docs: List[Document]\n",
    "    verdict: str\n",
    "    reason: str\n",
    "\n",
    "    strips: List[str]\n",
    "    kept_strips: List[str]\n",
    "    refined_context: str\n",
    "\n",
    "    web_docs: List[Document]\n",
    "\n",
    "    web_query: str          # âœ… NEW\n",
    "    \n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f53a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state: State) -> State:\n",
    "    q = state[\"question\"]\n",
    "    return {\"docs\": retriever.invoke(q)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Score-based doc evaluator\n",
    "# -----------------------------\n",
    "class DocEvalScore(BaseModel):\n",
    "    score: float\n",
    "    reason: str\n",
    "\n",
    "doc_eval_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a strict retrieval evaluator for RAG.\\n\"\n",
    "            \"You will be given ONE retrieved chunk and a question.\\n\"\n",
    "            \"Return a relevance score in [0.0, 1.0].\\n\"\n",
    "            \"- 1.0: chunk alone is sufficient to answer fully/mostly\\n\"\n",
    "            \"- 0.0: chunk is irrelevant\\n\"\n",
    "            \"Be conservative with high scores.\\n\"\n",
    "            \"Also return a short reason.\\n\"\n",
    "            \"Output JSON only.\",\n",
    "        ),\n",
    "        (\"human\", \"Question: {question}\\n\\nChunk:\\n{chunk}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "doc_eval_chain = doc_eval_prompt | llm.with_structured_output(DocEvalScore)\n",
    "\n",
    "def eval_each_doc_node(state: State) -> State:\n",
    "    q = state[\"question\"]\n",
    "    scores: List[float] = []\n",
    "    good: List[Document] = []\n",
    "\n",
    "    for d in state[\"docs\"]:\n",
    "        out = doc_eval_chain.invoke({\"question\": q, \"chunk\": d.page_content})\n",
    "        scores.append(out.score)\n",
    "\n",
    "        if out.score > LOWER_TH:\n",
    "            good.append(d)\n",
    "\n",
    "    if any(s > UPPER_TH for s in scores):\n",
    "        return {\n",
    "            \"good_docs\": good,\n",
    "            \"verdict\": \"CORRECT\",\n",
    "            \"reason\": f\"At least one retrieved chunk scored > {UPPER_TH}.\",\n",
    "        }\n",
    "\n",
    "    if len(scores) > 0 and all(s < LOWER_TH for s in scores):\n",
    "        why = \"No chunk was sufficient.\"\n",
    "        return {\n",
    "            \"good_docs\": [],\n",
    "            \"verdict\": \"INCORRECT\",\n",
    "            \"reason\": f\"All retrieved chunks scored < {LOWER_TH}. {why}\",\n",
    "        }\n",
    "\n",
    "    why = \"Mixed relevance signals.\"\n",
    "    return {\n",
    "        \"good_docs\": good,\n",
    "        \"verdict\": \"AMBIGUOUS\",\n",
    "        \"reason\": f\"No chunk scored > {UPPER_TH}, but not all were < {LOWER_TH}. {why}\",\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
